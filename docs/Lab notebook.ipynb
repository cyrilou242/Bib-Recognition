{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Vision Dossard Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prise en main "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mise en place de l'authentification API de Google Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clé API a été déclarée en tant que telle suivant la documentation ici: https://cloud.google.com/docs/authentication/getting-started?hl=fr#auth-cloud-implicit-python\n",
    "\n",
    "On teste si la connexion est bien acceptée en faisant un test sur une image en local.\n",
    "Je n'ai pas réussi à utiliser une variable globale GOOGLE_APPLICATION_CREDENTIALS sur mon Unix, l'Auth n'arrive pas à la récupérer. (me dit que le fichier est inexistant). Je définis donc la variable après mes imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "path\n",
      "tree\n",
      "woody plant\n",
      "trail\n",
      "mountain bike\n",
      "woodland\n",
      "duathlon\n",
      "ultramarathon\n",
      "bicycle racing\n",
      "bicycle\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import vision\n",
    "from google.cloud.vision import types\n",
    "\n",
    "\n",
    "# Instantiates a client\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "# The name of the image file to annotate\n",
    "file_name = os.path.join(\n",
    "    os.path.dirname(\"__file__\"),\n",
    "    'resources/2.png')\n",
    "\n",
    "# Loads the image into memory\n",
    "with io.open(file_name, 'rb') as image_file:\n",
    "    content = image_file.read()\n",
    "\n",
    "image = types.Image(content=content)\n",
    "\n",
    "# Performs label detection on the image file\n",
    "response = client.label_detection(image=image)\n",
    "labels = response.label_annotations\n",
    "\n",
    "print('Labels:')\n",
    "for label in labels:\n",
    "    print(label.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait le même test avec une autre rédaction, sous forme de call JSON. On va aussi chercher l'image sur un bucket google plutôt que en local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "path\n",
      "tree\n",
      "woody plant\n",
      "trail\n",
      "mountain bike\n",
      "woodland\n",
      "duathlon\n",
      "ultramarathon\n",
      "bicycle racing\n",
      "bicycle\n"
     ]
    }
   ],
   "source": [
    "client2 = vision.ImageAnnotatorClient()\n",
    "response2 = client2.annotate_image({'image': {'source': {'image_uri': 'gs://bib_sample/2.png'}},'features': [{'type': vision.enums.Feature.Type.LABEL_DETECTION}],})\n",
    "labels = response2.label_annotations\n",
    "print('Labels:')\n",
    "for label in labels:\n",
    "    print(label.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello Text Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1er test : reconnaissance de texte sur image full size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On essaie maintenant de faire un call pour repérer du texte. Voici l'image sur laquelle on travaille:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/2.png\" width=\"400\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textes:\n"
     ]
    }
   ],
   "source": [
    "client3 = vision.ImageAnnotatorClient()\n",
    "response3 = client3.annotate_image({'image': {'source': {'image_uri': 'gs://bib_sample/2.png'}},'features': [{'type': vision.enums.Feature.Type.TEXT_DETECTION}],})\n",
    "texts = response3.text_annotations\n",
    "print('Textes:')\n",
    "for text in texts:\n",
    "    print('\\n\"{}\"'.format(text.description))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code ne renvoie rien, bizarre...On voudrait qu'il y ait 464. On testera donc ensuite sur un zoom sur les dossards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2ème test : reconnaissance de texte sur image zoomée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/2-zoom.png\" width=\"400\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textes:\n",
      "\n",
      "\"464\n",
      "\"\n",
      "bounds: (166,113),(240,113),(240,150),(166,150)\n",
      "\n",
      "\"464\"\n",
      "bounds: (167,113),(240,116),(238,150),(166,147)\n"
     ]
    }
   ],
   "source": [
    "client4 = vision.ImageAnnotatorClient()\n",
    "response4 = client4.annotate_image({'image': {'source': {'image_uri': 'gs://bib_sample/2-zoom.png'}},'features': [{'type': vision.enums.Feature.Type.TEXT_DETECTION}],})\n",
    "texts = response4.text_annotations\n",
    "print('Textes:')\n",
    "for text in texts:\n",
    "        print('\\n\"{}\"'.format(text.description))\n",
    "\n",
    "        vertices = (['({},{})'.format(vertex.x, vertex.y)\n",
    "                    for vertex in text.bounding_poly.vertices])\n",
    "\n",
    "        print('bounds: {}'.format(','.join(vertices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'IA a bien repéré 464! Par contre le dossard a été repérée 2 fois, a des positions très semblables. \n",
    "Soit au moins 2 points à cruncher: \n",
    "- trouver un dossard avant d'essayer de le lire.\n",
    "- vérifier les doublons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3ème test: reconnaissance sur le crop hint de google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une feature de crop hint existe dans l'API. Elle tend à renvoyer la partie importante d'ume image. On va réaliser un crop hint de l'image full size, puis réaliser une détection de texte sur l'image cropé ainsi. On défini les fonctions de crop à l'aide de la documentation donnée ici : https://cloud.google.com/vision/docs/crop-hints?hl=fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def get_crop_hint(path):\n",
    "    \"\"\"Detect crop hints on a single image and return the first result.\"\"\"\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    with io.open(path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = types.Image(content=content)\n",
    "\n",
    "    crop_hints_params = types.CropHintsParams(aspect_ratios=[1.77])\n",
    "    image_context = types.ImageContext(crop_hints_params=crop_hints_params)\n",
    "\n",
    "    response = client.crop_hints(image=image, image_context=image_context)\n",
    "    hints = response.crop_hints_annotation.crop_hints\n",
    "\n",
    "    # Get bounds for the first crop hint using an aspect ratio of 1.77.\n",
    "    vertices = hints[0].bounding_poly.vertices\n",
    "\n",
    "    return vertices\n",
    "\n",
    "def get_crop_hint_Bucket(bucketPath):\n",
    "    \"\"\"Detect crop hints on a single image and return the first result.\"\"\"\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    \n",
    "    source = types.ImageSource(bucketPath)\n",
    "\n",
    "    image = types.Image(source=source)\n",
    "\n",
    "    crop_hints_params = types.CropHintsParams(aspect_ratios=[1.77])\n",
    "    image_context = types.ImageContext(crop_hints_params=crop_hints_params)\n",
    "\n",
    "    response = client.crop_hints(image=image, image_context=image_context)\n",
    "    hints = response.crop_hints_annotation.crop_hints\n",
    "\n",
    "    # Get bounds for the first crop hint using an aspect ratio of 1.77.\n",
    "    vertices = hints[0].bounding_poly.vertices\n",
    "\n",
    "    return vertices\n",
    "\n",
    "def draw_hint(image_file):\n",
    "    \"\"\"Draw a border around the image using the hints in the vector list.\"\"\"\n",
    "    vects = get_crop_hint(image_file)\n",
    "\n",
    "    im = Image.open(image_file)\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    draw.polygon([\n",
    "        vects[0].x, vects[0].y,\n",
    "        vects[1].x, vects[1].y,\n",
    "        vects[2].x, vects[2].y,\n",
    "        vects[3].x, vects[3].y], None, 'red')\n",
    "    im.save('output-hint.jpg', 'JPEG')\n",
    "    \n",
    "def crop_to_hint(image_file):\n",
    "    \"\"\"Crop the image using the hints in the vector list.\"\"\"\n",
    "    vects = get_crop_hint(image_file)\n",
    "\n",
    "    im = Image.open(image_file)\n",
    "    im2 = im.crop([vects[0].x, vects[0].y,\n",
    "                  vects[2].x - 1, vects[2].y - 1])\n",
    "    im2.save('output-crop.jpg', 'JPEG')\n",
    "    \n",
    "def draw_hint_PNG(image_file):\n",
    "    \"\"\"Draw a border around the image using the hints in the vector list.\"\"\"\n",
    "    vects = get_crop_hint(image_file)\n",
    "\n",
    "    im = Image.open(image_file)\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    draw.polygon([\n",
    "        vects[0].x, vects[0].y,\n",
    "        vects[1].x, vects[1].y,\n",
    "        vects[2].x, vects[2].y,\n",
    "        vects[3].x, vects[3].y], None, 'red')\n",
    "    im.save('output-hint4.png', 'PNG')\n",
    "    \n",
    "def crop_to_hint_PNG(image_file):\n",
    "    \"\"\"Crop the image using the hints in the vector list.\"\"\"\n",
    "    vects = get_crop_hint(image_file)\n",
    "\n",
    "    im = Image.open(image_file)\n",
    "    im2 = im.crop([vects[0].x, vects[0].y,\n",
    "                  vects[2].x - 1, vects[2].y - 1])\n",
    "    im2.save('output-crop4.png', 'PNG')\n",
    "\n",
    "    \n",
    "##Parser de fonction main / call Pas utile pour le moment    \n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument('image_file', help='The image you\\'d like to crop.')\n",
    "#parser.add_argument('mode', help='Set to \"crop\" or \"draw\".')\n",
    "#args = parser.parse_args()\n",
    "\n",
    "#parser = argparse.ArgumentParser()\n",
    "\n",
    "#if args.mode == 'crop':\n",
    "#    crop_to_hint(args.image_file)\n",
    "#elif args.mode == 'draw':\n",
    "#    draw_hint(args.image_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fonctions sont bien définies. On teste sur une image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# fonction pour afficher un cadre autour du hint: \n",
    "draw_hint_PNG('resources/4.png')\n",
    "\n",
    "# fonction pour créer un crop\n",
    "crop_to_hint_PNG('resources/4.png')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats sont plutôt médiocres sur les images testées: \n",
    "<img src=\"crop-hint-output/output-hint4.png\" width=\"400\" height=\"400\" /><img src=\"crop-hint-output/output-hint1.png\" width=\"400\" height=\"400\" /><img src=\"crop-hint-output/output-hint2.png\" width=\"400\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va donc devoir essayer de répérer les personnes plutot qu'utiliser le hint. On sait qu'une personne doit porter un dossard. Il faudra essayer de faire le focus sur le corps, et si possible sur le buste, et ne pas se contenter de la tête comme semblent le faire les calls basiques de l'API Vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Détection de corps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call sur une partie d'une image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suite à la détection des corps, ou de la zone ou des corps sont présents, on peut faire un call sur cette zone de l'image. On va comparer la detection de texte sur une partie d'image et sur une image complète."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textes:\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Detection sur image entière:\n",
    "client10 = vision.ImageAnnotatorClient()\n",
    "response10 = client10.annotate_image({'image': {'source': {'image_uri': 'gs://bib_sample/2.png'}},'features': [{'type': vision.enums.Feature.Type.TEXT_DETECTION}],})\n",
    "texts = response10.text_annotations\n",
    "print('Textes:')\n",
    "for text in texts:\n",
    "    print('\\n\"{}\"'.format(text.description))\n",
    "print('Fin des Textes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La recherche de texte sur l'image 2 complète ne renvoie rien. On essaie maintenant sur une partie de l'image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'image'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6283092e9f20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Detection sur partie de l'image:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclient11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageAnnotatorClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresponse11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"requests\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"source\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"imageUri\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"gs://bib_sample/2.png\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"TEXT_DETECTION\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"imageContext\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"latLongRect\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"maxLatLng\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"latitude\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"longitude\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"minLatLng\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"latitude\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"longitude\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m170\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Textes:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/google/cloud/vision_helpers/__init__.py\u001b[0m in \u001b[0;36mannotate_image\u001b[0;34m(self, request, options)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \"\"\"\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# If the image is a file handler, set the content.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprotobuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'read'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mimg_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/google/gax/utils/protobuf.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(pb_or_dict, key, default)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# a \"not found\" case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0m_SENTINEL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# If a subkey exists, call this method recursively against the answer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'image'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#Detection sur partie de l'image:\n",
    "client11 = vision.ImageAnnotatorClient()\n",
    "response11 = client11.annotate_image({\"requests\": [{\"image\": {\"source\": {\"imageUri\": \"gs://bib_sample/2.png\"}},\"features\": [{\"type\": \"TEXT_DETECTION\"}],\"imageContext\": {\"latLongRect\": {\"maxLatLng\": {\"latitude\": 50,\"longitude\": 0},\"minLatLng\": {\"latitude\": -70,\"longitude\": -170}}}}]})\n",
    "texts = response11.text_annotations\n",
    "print('Textes:')\n",
    "for text in texts:\n",
    "    print('\\n\"{}\"'.format(text.description))\n",
    "print('Fin des Textes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Traitement d'une liste d'image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sujet traité par Pop's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Il est tout à fait possible de laisser les image en privé dans le bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
